<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-new.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-new.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-new.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo-new.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习 线性回归 逻辑回归,">










<meta name="description" content="参考：吴恩达老师的机器学习课程个人笔记  引言监督学习监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。 回归问题：预测的结果是连续的，如预测房价、未来的天气情况等等。分类问题：预测的结果是离散的，例如预测明天天气-阴，晴，雨。 无监督学习据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。 单变量线性回归模型表示">
<meta name="keywords" content="机器学习 线性回归 逻辑回归">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达机器学习笔记">
<meta property="og:url" content="http://yoursite.com/2019/10/24/吴恩达机器学习笔记/index.html">
<meta property="og:site_name" content="Xiaoyi">
<meta property="og:description" content="参考：吴恩达老师的机器学习课程个人笔记  引言监督学习监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。 回归问题：预测的结果是连续的，如预测房价、未来的天气情况等等。分类问题：预测的结果是离散的，例如预测明天天气-阴，晴，雨。 无监督学习据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。 单变量线性回归模型表示">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-10-28T14:57:04.583Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="吴恩达机器学习笔记">
<meta name="twitter:description" content="参考：吴恩达老师的机器学习课程个人笔记  引言监督学习监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。 回归问题：预测的结果是连续的，如预测房价、未来的天气情况等等。分类问题：预测的结果是离散的，例如预测明天天气-阴，晴，雨。 无监督学习据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。 单变量线性回归模型表示">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/10/24/吴恩达机器学习笔记/">





  <title>吴恩达机器学习笔记 | Xiaoyi</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://your-url" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Xiaoyi</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/24/吴恩达机器学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyi Jin">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoyi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">吴恩达机器学习笔记</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-24T19:53:21+08:00">
                2019-10-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/吴恩达机器学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">吴恩达机器学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>参考：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">吴恩达老师的机器学习课程个人笔记 </a></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。</p>
<p>回归问题：预测的结果是连续的，如预测房价、未来的天气情况等等。<br>分类问题：预测的结果是离散的，例如预测明天天气-阴，晴，雨。</p>
<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。</p>
<h1 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h1><h3 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h3><p>$h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$</p>
<p>其中 $h$ 代表学习算法的解决方案或函数也称为假设。</p>
<h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>$J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$</p>
<p>目标是选择出使得建模误差的平方和能够最小的模型参数，即使得代价函数最小的参数。</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>${\theta_{j} }:={\theta_{j} }-\alpha \frac{\partial }{\partial {\theta_{j} } }J\left(\theta \right)$</p>
<p>其中𝑎是学习率（learning rate），它决定了沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。</p>
<h3 id="梯度下降的线性回归"><a href="#梯度下降的线性回归" class="headerlink" title="梯度下降的线性回归"></a>梯度下降的线性回归</h3><p>对之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：</p>
<p>$\frac{\partial }{\partial { {\theta }_{j} } }J({ {\theta }_{0} },{ {\theta }_{1} })=\frac{\partial }{\partial { {\theta }_{j} } }\frac{1}{2m}{ {\sum\limits_{i=1}^{m}{\left({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)} }^{2} }$</p>
<p>$j=0$  时：$\frac{\partial }{\partial { {\theta }_{0} } }J({ {\theta }_{0} },{ {\theta }_{1} })=\frac{1}{m}{ {\sum\limits_{i=1}^{m}{\left({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)} } }$</p>
<p>$j=1$  时：$\frac{\partial }{\partial { {\theta }_{1} } }J({ {\theta }_{0} },{ {\theta }_{1} })=\frac{1}{m}\sum\limits_{i=1}^{m}{\left(\left({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)\cdot { {x}^{(i)} } \right)}$</p>
<p>则算法为：</p>
<p><strong>Repeat {</strong></p>
<p>​                ${\theta_{0} }:={\theta_{0} }-a\frac{1}{m}\sum\limits_{i=1}^{m}{ \left({ {h}_{\theta } }({ {x}^{(i)} })-{  {y}^{(i)} } \right)}$</p>
<p>​                ${\theta_{1} }:={\theta_{1} }-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)\cdot { {x}^{(i)} } \right)}$</p>
<p>​               <strong>}</strong></p>
<h1 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h1><h3 id="模型表示-1"><a href="#模型表示-1" class="headerlink" title="模型表示"></a>模型表示</h3><p>$h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2} }+…+{\theta_{n} }{x_{n} }$</p>
<p>引入$x_{0}=1$，公式可以简化为：$h_{\theta} \left( x \right)={\theta^{T} }X$。</p>
<h3 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h3><p>$J\left({\theta_{0} },{\theta_{1} }…{\theta_{n} } \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ { {\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2} } }$</p>
<h3 id="梯度下降-1"><a href="#梯度下降-1" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><strong>Repeat {</strong></p>
<p>${\theta_{j} }:={\theta_{j} }-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)\cdot { {x_{j} }^{(i)} } \right)}$</p>
<p>(<strong>simultaneously update all $\theta_{j}$</strong> )</p>
<p>​               <strong>}</strong></p>
<h3 id="梯度下降的实践：特征缩放、学习率"><a href="#梯度下降的实践：特征缩放、学习率" class="headerlink" title="梯度下降的实践：特征缩放、学习率"></a>梯度下降的实践：特征缩放、学习率</h3><p>优化时特征具有相近的尺度，梯度下降算法收敛更快，方法时将特征尽量缩放到-1到1之间。特征缩放的简单方法为标准化：${ {x}_{n} }=\frac{ { {x}_{n} }-{ {\mu}_{n} } }{ { {s}_{n} } }$，其中 ${\mu_{n} }$是平均值，${s_{n} }$是标准差。</p>
<p>如果学习率$a$过小，则达到收敛所需的迭代次数会非常高；如果学习率$a$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。通常可以考虑尝试些学习率：$\alpha=0.01，0.03，0.1，0.3，1，3，10$。</p>
<h3 id="特征和多项式回归"><a href="#特征和多项式回归" class="headerlink" title="特征和多项式回归"></a>特征和多项式回归</h3><p>线性回归并不适用于所有数据，有时需要曲线来适应数据，如三次方模型：$h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2}^2}+{\theta_{3} }{x_{3}^3}$ 。可以令：${ {x}_{2} }=x_{2}^{2},{ {x}_{3} }=x_{3}^{3}$，模型就转化为线性回归模型。</p>
<h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>正规方程是通过令代价函数导数等于0来找出使得代价函数最小的参数的：$\frac{\partial}{\partial{\theta_{j} } }J\left({\theta_{j} } \right)=0$ 。</p>
<p>假设训练集特征矩阵为 $X$（包含了 ${ {x}_{0} }=1$）并且训练集结果为向量 $y$，则利用正规方程解出向量 $\theta ={ {\left({X^T}X \right)}^{-1} }{X^{T} }y$ 。</p>
<p>梯度下降与正规方程的比较：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习率$\alpha$</td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>当特征数量$n$大时也能较好适用</td>
<td>需要计算${ {\left({ {X}^{T} }X \right)}^{-1} }$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O\left({ {n}^{3} } \right)$，通常来说当$n$小于10000 时还是可以接受的</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型</td>
</tr>
</tbody>
</table>
</div>
<p>当矩阵$X’X$不可逆时，可以使用广义逆。</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h3 id="模型表示-2"><a href="#模型表示-2" class="headerlink" title="模型表示"></a>模型表示</h3><p>$h_\theta \left( x \right)=g\left(\theta^{T}X \right)$</p>
<p>$g$ 代表逻辑函数（<strong>logistic function</strong>)是一个常用的逻辑函数为<strong>S</strong>形函数（<strong>Sigmoid function</strong>），公式为： $g\left( z \right)=\frac{1}{1+{  {e}^{-z} } }$。</p>
<p>$h_\theta \left( x \right)$的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性（<strong>estimated probablity</strong>）即$h_\theta \left( x \right)=P\left( y=1|x;\theta \right)$。</p>
<h3 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h3><p>${\theta^{T} }x=0$，这条线是模型的分界线，将预测为1的区域和预测为 0的区域分隔开。</p>
<h3 id="代价函数-2"><a href="#代价函数-2" class="headerlink" title="代价函数"></a>代价函数</h3><p>$J\left( \theta  \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{ {Cost}\left({h_\theta}\left({x}^{\left( i \right)} \right),{y}^{\left( i \right)} \right)}$</p>
<p>其中：$Cost\left({h_\theta}\left( x \right),y \right)=-y\times log\left({h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$</p>
<h3 id="梯度下降-2"><a href="#梯度下降-2" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><strong>Repeat</strong> {<br>$\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$<br>(<strong>simultaneously update all</strong> )<br>}</p>
<p>求导后得到：</p>
<p><strong>Repeat</strong> {<br>${\theta_{j} }:={\theta_{j} }-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)\cdot { {x_{j} }^{(i)} } \right)}$<br><strong>(simultaneously update all</strong> )<br>}</p>
<p>推导过程：</p>
<p>$J\left( \theta  \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{ {y}^{(i)} }\log \left({h_\theta}\left({ {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left({ {x}^{(i)} } \right) \right)]}$<br>考虑：<br>${h_\theta}\left({ {x}^{(i)} } \right)=\frac{1}{1+{ {e}^{-{\theta^T}{ {x}^{(i)} } } } }$<br>则：<br>${ {y}^{(i)} }\log \left({h_\theta}\left({ {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left({ {x}^{(i)} } \right) \right)$<br>$={ {y}^{(i)} }\log \left( \frac{1}{1+{ {e}^{-{\theta^T}{ {x}^{(i)} } } } } \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-\frac{1}{1+{ {e}^{-{\theta^T}{ {x}^{(i)} } } } } \right)$<br>$=-{ {y}^{(i)} }\log \left( 1+{ {e}^{-{\theta^T}{ {x}^{(i)} } } } \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1+{ {e}^{ {\theta^T}{ {x}^{(i)} } } } \right)$</p>
<p>所以：<br>$\frac{\partial }{\partial {\theta_{j} } }J\left( \theta  \right)=\frac{\partial }{\partial {\theta_{j} } }[-\frac{1}{m}\sum\limits_{i=1}^{m}{[-{ {y}^{(i)} }\log \left( 1+{ {e}^{-{\theta^{T} }{ {x}^{(i)} } } } \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1+{ {e}^{ {\theta^{T} }{ {x}^{(i)} } } } \right)]}]$<br>$=-\frac{1}{m}\sum\limits_{i=1}^{m}{[-{ {y}^{(i)} }\frac{-x_{j}^{(i)}{ {e}^{-{\theta^{T} }{ {x}^{(i)} } } } }{1+{ {e}^{-{\theta^{T} }{ {x}^{(i)} } } } }-\left( 1-{ {y}^{(i)} } \right)\frac{x_j^{(i)}{ {e}^{ {\theta^T}{ {x}^{(i)} } } } }{1+{ {e}^{ {\theta^T}{ {x}^{(i)} } } } } }]$<br>$=-\frac{1}{m}\sum\limits_{i=1}^{m}{ {y}^{(i)} }\frac{x_j^{(i)} }{1+{ {e}^{ {\theta^T}{ {x}^{(i)} } } } }-\left( 1-{ {y}^{(i)} } \right)\frac{x_j^{(i)}{ {e}^{ {\theta^T}{ {x}^{(i)} } } } }{1+{ {e}^{ {\theta^T}{ {x}^{(i)} } } } }]$<br>$=-\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{ { {y}^{(i)} }x_j^{(i)}-x_j^{(i)}{ {e}^{ {\theta^T}{ {x}^{(i)} } } }+{ {y}^{(i)} }x_j^{(i)}{ {e}^{ {\theta^T}{ {x}^{(i)} } } } }{1+{ {e}^{ {\theta^T}{ {x}^{(i)} } } } } }$<br>$=-\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{ { {y}^{(i)} }\left( 1\text{+}{ {e}^{ {\theta^T}{ {x}^{(i)} } } } \right)-{ {e}^{ {\theta^T}{ {x}^{(i)} } } } }{1+{ {e}^{ {\theta^T}{ {x}^{(i)} } } } }x_j^{(i)} }$<br>$=-\frac{1}{m}\sum\limits_{i=1}^{m}{({ {y}^{(i)} }-\frac{ { {e}^{ {\theta^T}{ {x}^{(i)} } } } }{1+{ {e}^{ {\theta^T}{ {x}^{(i)} } } } })x_j^{(i)} }$<br>$=-\frac{1}{m}\sum\limits_{i=1}^{m}{({ {y}^{(i)} }-\frac{1}{1+{ {e}^{-{\theta^T}{ {x}^{(i)} } } } })x_j^{(i)} }$<br>$=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{ {y}^{(i)} }-{h_\theta}\left({ {x}^{(i)} } \right)]x_j^{(i)} }$<br>$=\frac{1}{m}\sum\limits_{i=1}^{m}{[{h_\theta}\left({ {x}^{(i)} } \right)-{ {y}^{(i)} }]x_j^{(i)} }$</p>
<p>其他优化方法：牛顿法，共轭梯度法，BFGS(变尺度法)，L-BFGS(限制内存BFGS)</p>
<h3 id="多类别分类"><a href="#多类别分类" class="headerlink" title="多类别分类"></a>多类别分类</h3><p>方法：将多分类问题转换为多个二分类问题。预测时选择一个让 $h_\theta^{\left( i \right)}\left( x \right)$ 最大的$ i$，即$\mathop{\max}\limits_i\,h_\theta^{\left( i \right)}\left( x \right)$。</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>欠拟合和过拟合：欠拟合不能很好的拟合训练集，过拟合过于强调拟合原始数据而不能很好的预测新数据。</p>
<p>过拟合的处理：</p>
<p>1.丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如<strong>PCA</strong>）</p>
<p>2.正则化。 保留所有的特征，但是减少参数的大小（<strong>magnitude</strong>）。</p>
<p>加入${\cal l}_{2}$正则化的代价函数：$J\left( \theta  \right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}{ { {({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })}^{2} }+\lambda \sum\limits_{j=1}^{n}{\theta_{j}^{2} }]}$</p>
<p>其中$\lambda $又称为正则化参数（<strong>Regularization Parameter</strong>）。 注：根据惯例，不对${\theta_{0} }$ 进行惩罚。</p>
<h3 id="正则化的线性回归模型"><a href="#正则化的线性回归模型" class="headerlink" title="正则化的线性回归模型"></a>正则化的线性回归模型</h3><p>代价函数：$J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{[({ {({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })}^{2} }+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2} })]}$</p>
<p>梯度下降算法：</p>
<p>$Repeat$  $until$  $convergence${</p>
<p>​                                                   ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{0}^{(i)} })$ </p>
<p>​                                                   ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{j}^{\left( i \right)} }+\frac{\lambda }{m}{\theta_j}]$ </p>
<p>​                                                             $for$ $j=1,2,…n$</p>
<p>​                                                   }</p>
<h3 id="正则化的逻辑回归模型"><a href="#正则化的逻辑回归模型" class="headerlink" title="正则化的逻辑回归模型"></a>正则化的逻辑回归模型</h3><p>代价函数：$J\left( \theta  \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}+\frac{\lambda }{2m}\sum\limits_{j=1}^{n}{\theta _{j}^{2} }$</p>
<p>梯度下降算法：</p>
<p>$Repeat$  $until$  $convergence${</p>
<p>​                                                   ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{0}^{(i)} })$</p>
<p>​                                                  ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{j}^{\left( i \right)} }+\frac{\lambda }{m}{\theta_j}]$</p>
<p>​                                                 $for$ $j=1,2,…n$</p>
<p>​                                                 }</p>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h3 id="模型表示-3"><a href="#模型表示-3" class="headerlink" title="模型表示"></a>模型表示</h3><p>两层神经网络：</p>
<p>${z}^{(2)}=\Theta^{(1)}{X}$</p>
<p>${a}^{(2)}=g({ {z}^{(2)} })$</p>
<p>${z}^{(3)}=\Theta^{(2)}{ {a}^{(2)} }$</p>
<p>$h_\theta(x)={ {a}^{(3)} }=g({ {z}^{(3)} })$</p>
<h3 id="代价函数-3"><a href="#代价函数-3" class="headerlink" title="代价函数"></a>代价函数</h3><p>二类分类：$S_L=0, y=0\, or\, 1$表示哪一类；</p>
<p>$\newcommand{\subk}[1]{ #1_k }$</p>
<script type="math/tex; mode=display">h_\theta\left(x\right)\in \mathbb{R}^{K}$$ $${\left({h_\theta}\left(x\right)\right)}_{i}={i}^{th} \text{output}</script><p>$K$类分类：$S_L=k, y_i = 1$表示分到第$i$类；$(k&gt;2)$</p>
<p>$h_\theta(x)$是一个维度为$K$的向量：$\newcommand{\subk}[1]{ #1_k }$</p>
<script type="math/tex; mode=display">h_\theta\left(x\right)\in \mathbb{R}^{K}$$ $${\left({h_\theta}\left(x\right)\right)}_{i}={i}^{th} \text{output}</script><p>$J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log \subk{(h_\Theta(x^{(i)}))} + \left( 1 - y_k^{(i)} \right) \log \left( 1- \subk{\left( h_\Theta \left( x^{(i)} \right) \right)} \right) \right] + \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_{l+1} } \left( \Theta_{ji}^{(l)} \right)^2$</p>
<h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>为了计算代价函数的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij} }J\left(\Theta\right)$，需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。</p>
<h3 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h3><p>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 $\theta$，计算出在 $\theta$-$\varepsilon $ 处和 $\theta$+$\varepsilon $ 的代价值（$\varepsilon $是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 $\theta$ 处的代价值。</p>
<p>一个只针对$\theta_1$进行检验的示例：</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial\theta_1}=\frac{J\left(\theta_1+\varepsilon_1,\theta_2,\theta_3...\theta_n \right)-J \left( \theta_1-\varepsilon_1,\theta_2,\theta_3...\theta_n \right)}{2\varepsilon}</script><h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>如果令所有的初始参数都为0，这将意味着第二层的所有激活单元都会有相同的值。同理，如果初始所有的参数都为一个非0的数，结果也是一样的。<br>所以神经网络中通常初始参数为正负ε之间的随机值。</p>
<h3 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h3><p>训练神经网络：</p>
<p>1.参数的随机初始化</p>
<p>2.利用正向传播方法计算所有的$h_{\theta}(x)$</p>
<p>3.编写计算代价函数 $J$ 的代码</p>
<p>4.利用反向传播方法计算所有偏导数</p>
<p>5.利用数值检验方法检验这些偏导数</p>
<p>6.使用优化算法来最小化代价函数</p>
<h1 id="应用机器学习的建议"><a href="#应用机器学习的建议" class="headerlink" title="应用机器学习的建议"></a>应用机器学习的建议</h1><h3 id="决定下一步做什么"><a href="#决定下一步做什么" class="headerlink" title="决定下一步做什么"></a>决定下一步做什么</h3><p>当运用训练好了的模型来预测未知数据的时候发现有较大的误差，下一步可以做什么？</p>
<p>1.获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</p>
<p>2.尝试减少特征的数量</p>
<p>3.尝试获得更多的特征</p>
<p>4.尝试增加多项式特征</p>
<p>5.尝试减少正则化程度$\lambda$</p>
<p>6.尝试增加正则化程度$\lambda$</p>
<h3 id="评估一个假设"><a href="#评估一个假设" class="headerlink" title="评估一个假设"></a>评估一个假设</h3><p>为了检验算法是否过拟合，将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。</p>
<p>在通过训练集让的模型学习得出其参数后，对测试集运用该模型，计算误差：</p>
<p>对于线性回归模型，利用测试集数据计算代价函数$J$</p>
<p>对于逻辑回归模型，除了可以利用测试数据集来计算代价函数外，还可以计算误分类的比例。</p>
<h3 id="模型选择和交叉验证集"><a href="#模型选择和交叉验证集" class="headerlink" title="模型选择和交叉验证集"></a>模型选择和交叉验证集</h3><p>使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集。</p>
<p>模型选择的方法为：</p>
<p>1.使用训练集训练出10个模型</p>
<p>2.用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</p>
<p>3.选取代价函数值最小的模型</p>
<p>4.用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）</p>
<h3 id="诊断偏差和方差"><a href="#诊断偏差和方差" class="headerlink" title="诊断偏差和方差"></a>诊断偏差和方差</h3><p><strong>Bias/variance</strong></p>
<p><strong>Training error:</strong>                               $J_{train}(\theta) = \frac{1}{2m}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$</p>
<p><strong>Cross Validation error:</strong>                $J_{cv}(\theta) = \frac{1}{2m_{cv} }\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)}_{cv})-y^{(i)}_{cv})^2$</p>
<p>对于训练集，当 $d$ 较小时，模型拟合程度更低，误差较大；随着 $d$ 的增长，拟合程度提高，误差减小。</p>
<p>对于交叉验证集，当 $d$ 较小时，模型拟合程度低，误差较大；但是随着 $d$ 的增长，误差呈现先减小后增大的趋势，转折点是模型开始过拟合训练数据集的时候。</p>
<p>训练集误差和交叉验证集误差近似时：偏差/欠拟合</p>
<p>交叉验证集误差远大于训练集误差时：方差/过拟合</p>
<h3 id="正则化和偏差-方差"><a href="#正则化和偏差-方差" class="headerlink" title="正则化和偏差/方差"></a>正则化和偏差/方差</h3><p>选择$\lambda$的值时，把数据分为训练集、交叉验证集和测试集。</p>
<p>1.使用训练集训练出12个不同程度正则化的模型</p>
<p>2.用12个模型分别对交叉验证集计算的出交叉验证误差</p>
<p>3.选择得出交叉验证误差<strong>最小</strong>的模型</p>
<p>4.运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上。</p>
<p>当 $\lambda$ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大</p>
<p>随着 $\lambda$ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</p>
<h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><p>学习曲线是将训练集误差和交叉验证集误差作为训练集样本数量（$m$）的函数绘制的图表。可以用学习曲线来判断某一个学习算法是否处于偏差、方差问题。</p>
<h3 id="决定下一步做什么-1"><a href="#决定下一步做什么-1" class="headerlink" title="决定下一步做什么"></a>决定下一步做什么</h3><ol>
<li><p>获得更多的训练样本——解决高方差</p>
</li>
<li><p>尝试减少特征的数量——解决高方差</p>
</li>
<li><p>尝试获得更多的特征——解决高偏差</p>
</li>
<li><p>尝试增加多项式特征——解决高偏差</p>
</li>
<li><p>尝试减少正则化程度λ——解决高偏差</p>
</li>
<li><p>尝试增加正则化程度λ——解决高方差</p>
</li>
</ol>
<h1 id="机器学习系统的设计"><a href="#机器学习系统的设计" class="headerlink" title="机器学习系统的设计"></a>机器学习系统的设计</h1><h3 id="首先要做什么"><a href="#首先要做什么" class="headerlink" title="首先要做什么"></a>首先要做什么</h3><p>1.收集更多的数据</p>
<p>2.基于数据开发的特征</p>
<p>3.基于目的开发算法</p>
<h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>构建一个学习算法的推荐方法为：</p>
<p>1.从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法</p>
<p>2.绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择</p>
<p>3.进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的样本，看看这些样本是否有某种系统化的趋势</p>
<h3 id="类偏斜的误差度量"><a href="#类偏斜的误差度量" class="headerlink" title="类偏斜的误差度量"></a>类偏斜的误差度量</h3><p>混淆矩阵：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th><strong>预测值</strong></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td><strong>Positive</strong></td>
<td><strong>Negtive</strong></td>
</tr>
<tr>
<td><strong>实际值</strong></td>
<td><strong>Positive</strong></td>
<td><strong>TP</strong></td>
<td><strong>FN</strong></td>
</tr>
<tr>
<td></td>
<td><strong>Negtive</strong></td>
<td><strong>FP</strong></td>
<td><strong>TN</strong></td>
</tr>
</tbody>
</table>
</div>
<p>查准率=<strong>TP/(TP+FP)</strong>。</p>
<p>查全率=<strong>TP/(TP+FN)</strong>。</p>
<h3 id="查准率和查全率之间的权衡"><a href="#查准率和查全率之间的权衡" class="headerlink" title="查准率和查全率之间的权衡"></a>查准率和查全率之间的权衡</h3><p><strong>F1 值</strong>（<strong>F1 Score</strong>）：${ {F}_{1} }Score:2\frac{PR}{P+R}$</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习-线性回归-逻辑回归/" rel="tag"># 机器学习 线性回归 逻辑回归</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/17/my-next/" rel="next" title="我的next主题配置">
                <i class="fa fa-chevron-left"></i> 我的next主题配置
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/28/正则表达式/" rel="prev" title="正则表达式">
                正则表达式 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Xiaoyi Jin">
            
              <p class="site-author-name" itemprop="name">Xiaoyi Jin</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#引言"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#监督学习"><span class="nav-number">1.0.1.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#无监督学习"><span class="nav-number">1.0.2.</span> <span class="nav-text">无监督学习</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#单变量线性回归"><span class="nav-number">2.</span> <span class="nav-text">单变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型表示"><span class="nav-number">2.0.1.</span> <span class="nav-text">模型表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数"><span class="nav-number">2.0.2.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降"><span class="nav-number">2.0.3.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降的线性回归"><span class="nav-number">2.0.4.</span> <span class="nav-text">梯度下降的线性回归</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#多变量线性回归"><span class="nav-number">3.</span> <span class="nav-text">多变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型表示-1"><span class="nav-number">3.0.1.</span> <span class="nav-text">模型表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数-1"><span class="nav-number">3.0.2.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降-1"><span class="nav-number">3.0.3.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降的实践：特征缩放、学习率"><span class="nav-number">3.0.4.</span> <span class="nav-text">梯度下降的实践：特征缩放、学习率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征和多项式回归"><span class="nav-number">3.0.5.</span> <span class="nav-text">特征和多项式回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正规方程"><span class="nav-number">3.0.6.</span> <span class="nav-text">正规方程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#逻辑回归"><span class="nav-number">4.</span> <span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型表示-2"><span class="nav-number">4.0.1.</span> <span class="nav-text">模型表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策边界"><span class="nav-number">4.0.2.</span> <span class="nav-text">决策边界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数-2"><span class="nav-number">4.0.3.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降-2"><span class="nav-number">4.0.4.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多类别分类"><span class="nav-number">4.0.5.</span> <span class="nav-text">多类别分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化"><span class="nav-number">4.0.6.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化的线性回归模型"><span class="nav-number">4.0.7.</span> <span class="nav-text">正则化的线性回归模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化的逻辑回归模型"><span class="nav-number">4.0.8.</span> <span class="nav-text">正则化的逻辑回归模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络"><span class="nav-number">5.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型表示-3"><span class="nav-number">5.0.1.</span> <span class="nav-text">模型表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数-3"><span class="nav-number">5.0.2.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播算法"><span class="nav-number">5.0.3.</span> <span class="nav-text">反向传播算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度检验"><span class="nav-number">5.0.4.</span> <span class="nav-text">梯度检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机初始化"><span class="nav-number">5.0.5.</span> <span class="nav-text">随机初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练神经网络"><span class="nav-number">5.0.6.</span> <span class="nav-text">训练神经网络</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#应用机器学习的建议"><span class="nav-number">6.</span> <span class="nav-text">应用机器学习的建议</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#决定下一步做什么"><span class="nav-number">6.0.1.</span> <span class="nav-text">决定下一步做什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#评估一个假设"><span class="nav-number">6.0.2.</span> <span class="nav-text">评估一个假设</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型选择和交叉验证集"><span class="nav-number">6.0.3.</span> <span class="nav-text">模型选择和交叉验证集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#诊断偏差和方差"><span class="nav-number">6.0.4.</span> <span class="nav-text">诊断偏差和方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化和偏差-方差"><span class="nav-number">6.0.5.</span> <span class="nav-text">正则化和偏差/方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习曲线"><span class="nav-number">6.0.6.</span> <span class="nav-text">学习曲线</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决定下一步做什么-1"><span class="nav-number">6.0.7.</span> <span class="nav-text">决定下一步做什么</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习系统的设计"><span class="nav-number">7.</span> <span class="nav-text">机器学习系统的设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#首先要做什么"><span class="nav-number">7.0.1.</span> <span class="nav-text">首先要做什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#误差分析"><span class="nav-number">7.0.2.</span> <span class="nav-text">误差分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#类偏斜的误差度量"><span class="nav-number">7.0.3.</span> <span class="nav-text">类偏斜的误差度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查准率和查全率之间的权衡"><span class="nav-number">7.0.4.</span> <span class="nav-text">查准率和查全率之间的权衡</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaoyi Jin</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
